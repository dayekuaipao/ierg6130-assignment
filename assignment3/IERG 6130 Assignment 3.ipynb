{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IERG 6130 Assignment 3: Policy Gradient\n",
    "\n",
    "*2019-2020 2nd term, IERG 6130: Reinforcement Learning and Beyond. Department of Information Engineering, The Chinese University of Hong Kong. Course Instructor: Professor ZHOU Bolei. Assignment author: PENG Zhenghao.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Student Name | Student ID |\n",
    "| :----: | :----: |\n",
    "| TYPE_YOUR_NAME_HERE | TYPE_YOUR_STUDENT_ID_HERE |\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welecome to the assignment 3 of our RL course. \n",
    "\n",
    "You need to go through this self-contained notebook, which contains many TODOs in part of the cells and has special `[TODO]` signs. You need to finish all TODOs. Some of them may be easy such as uncommenting a line, some of them may be difficult such as implementing a function. You can find them by searching the `[TODO]` symbol. However, we suggest you to go through the notebook step by step, which would give you a better understanding of the content.\n",
    "\n",
    "You are encouraged to add more code on extra cells at the end of the each section to investigate the problems you think interesting. At the end of the file, we left a place for you to optionaly write comments.\n",
    "\n",
    "Please report any code bugs to us via cuhkrlcourse@googlegroups.com or via github issue. Before you submission, remember to check the submit instruction at the directory of this assignment and make sure the required contents are included in your submission.\n",
    "\n",
    "Before you get start, remember to follow the instruction at https://github.com/cuhkrlcourse/ierg6130 to setup your environment.\n",
    "\n",
    "We will cover the following knowledege in this assignment:\n",
    "\n",
    "1. Basic policy gradient method\n",
    "2. Policy gradient with baseline\n",
    "3. Actor-critic framework\n",
    "\n",
    "You need to install some packages.\n",
    "1. Install `yaml` package via `pip install pyyaml` to print training information.\n",
    "2. Install `pandas` via `pip install pandas` to organize the learning progress.\n",
    "3. Install `matplotlib` via `pip install matplotlib` to visualize the learning result.\n",
    "4. Install `seaborn` via `pip install seaborn` to visualize the learning result (beautiful than matplotlib)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Section 1: Basic reinforcement learning pipeline\n",
    "\n",
    "(5 / 100 points)\n",
    "\n",
    "In this section, we will prepare several functions for evaulation, training RL algorithms. We will also build an `AbstractTrainer` class used as a general framework which left blanks for policy gradient methods.\n",
    "\n",
    "A essential difference of this assignment compared to the previous is that: the sampling and the optimization are splited into two phase. In each training iteration, the agent first collects a batch of samples followed by conducting the policy optimization. This modification allows us to have a clear view on the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "def run(trainer_cls, config=None, reward_threshold=None):\n",
    "    \"\"\"Run the trainer and report progress, agnostic to the class of trainer\n",
    "    :param trainer_cls: A trainer class \n",
    "    :param config: A dict\n",
    "    :param reward_threshold: the reward threshold to break the training\n",
    "    :return: The trained trainer and a dataframe containing learning progress\n",
    "    \"\"\"\n",
    "    assert inspect.isclass(trainer_cls)\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    trainer = trainer_cls(config)\n",
    "    config = trainer.config\n",
    "    start = now = time.time()\n",
    "    stats = []\n",
    "    print(\"=== {} {} Training Start ===\".format(trainer_cls.__name__, config[\"env_name\"]))\n",
    "    pretty_print({\"Config\": config})\n",
    "    for i in range(config['max_iteration'] + 1):\n",
    "        stat = trainer.train()\n",
    "        stats.append(stat or {})\n",
    "        if i % config['evaluate_interval'] == 0 or \\\n",
    "                i == config[\"max_iteration\"]:\n",
    "            reward = trainer.evaluate(config[\"evaluate_num_episodes\"])\n",
    "            stat[\"evaluate_reward\"] = reward\n",
    "            print(\"=== {} {} Iteration {} ===\".format(\n",
    "                trainer_cls.__name__, config[\"env_name\"], i)\n",
    "            )\n",
    "            pretty_print({\"Training Progress\": stat})\n",
    "            now = time.time()\n",
    "        if reward_threshold is not None and reward > reward_threshold:\n",
    "            print(\"In {} iteration, current mean episode reward {:.3f} is \"\n",
    "                  \"greater than reward threshold {}. Congratulation! Now we \"\n",
    "                  \"exit the training process.\".format(\n",
    "                i, reward, reward_threshold))\n",
    "            break\n",
    "    stats = pd.DataFrame(stats)\n",
    "    return trainer, stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "default_config = dict(\n",
    "    env_name=\"CartPole-v0\",\n",
    "    \n",
    "    max_iteration=1000,\n",
    "    max_episode_length=1000,\n",
    "    \n",
    "    train_batch_size=1000,\n",
    "    gamma=0.99,\n",
    "    learning_rate=0.01,\n",
    "    seed=0,\n",
    "    \n",
    "    evaluate_interval=10,\n",
    "    evaluate_num_episodes=50\n",
    ")\n",
    "\n",
    "\n",
    "class AbstractTrainer:\n",
    "    \"\"\"This is the abstract class for value-based RL trainer. We will inherent\n",
    "    the specify algorithm's trainer from this abstract class, so that we can\n",
    "    reuse the codes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        self.config = check_and_merge_config(config or {}, default_config)\n",
    "\n",
    "        # Create the environment\n",
    "        self.env_name = self.config['env_name']\n",
    "        self.env = gym.make(self.env_name)\n",
    "        if \"ram\" in self.env_name:  # Special process for Atari games\n",
    "            self.env = wrap_deepmind_ram(self.env)\n",
    "\n",
    "        # Apply the random seed\n",
    "        self.seed = self.config[\"seed\"]\n",
    "        seed_everything(self.seed)\n",
    "        self.env.seed(self.seed)\n",
    "\n",
    "        assert isinstance(self.env.observation_space,\n",
    "                          gym.spaces.Box), self.env.observation_space\n",
    "        self.obs_dim = self.env.observation_space.shape[0]\n",
    "        self.discrete_obs = True\n",
    "\n",
    "        assert isinstance(self.env.action_space, gym.spaces.discrete.Discrete)\n",
    "        self.act_dim = self.env.action_space.n\n",
    "\n",
    "        self.iteration = 0\n",
    "        self.start_time = time.time()\n",
    "        self.iteration_time = self.start_time\n",
    "        self.total_timesteps = 0\n",
    "        self.total_episodes = 0\n",
    "\n",
    "        # build the model\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # ===== Helper functions =====\n",
    "    def evaluate(self, num_episodes=10, render=False):\n",
    "        \"\"\"Evaluate the agents for num_episodes episodes.\"\"\"\n",
    "        return evaluate_agent(self, self.env, num_episodes, render)\n",
    "\n",
    "    def to_tensor(self, array):\n",
    "        \"\"\"Preprocess the observation.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def compute_action(self, observation):\n",
    "        \"\"\"Return the action, which can feed to self.env.step immediately.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def compute_values(self, observation):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def compute_log_probs(self, observation):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # ===== Training-related functions =====\n",
    "    def collect_samples(self):\n",
    "        \"\"\"Here we define the pipeline to collect sample even though\n",
    "        any specify functions are not implemented yet.\n",
    "        \"\"\"\n",
    "        iter_timesteps = 0\n",
    "        iter_episodes = 0\n",
    "        episode_lens = []\n",
    "        episode_rewards = []\n",
    "        episode_obs_list = []\n",
    "        episode_act_list = []\n",
    "        episode_reward_list = []\n",
    "        while iter_timesteps <= self.config[\"train_batch_size\"]:\n",
    "            obs_list, act_list, reward_list = [], [], []\n",
    "            obs = self.env.reset()\n",
    "            steps = 0\n",
    "            episode_reward = 0\n",
    "            while True:\n",
    "                act = self.compute_action(obs)\n",
    "                next_obs, reward, done, _ = self.env.step(act)\n",
    "\n",
    "                obs_list.append(obs)\n",
    "                act_list.append(act)\n",
    "                reward_list.append(reward)\n",
    "\n",
    "                obs = next_obs.copy()\n",
    "                steps += 1\n",
    "                episode_reward += reward\n",
    "                if done or steps > self.config[\"max_episode_length\"]:\n",
    "                    break\n",
    "            iter_timesteps += steps\n",
    "            iter_episodes += 1\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_lens.append(steps)\n",
    "            episode_obs_list.append(np.array(obs_list, dtype=np.float32))\n",
    "            episode_act_list.append(np.array(act_list, dtype=np.float32))\n",
    "            episode_reward_list.append(np.array(reward_list, dtype=np.float32))\n",
    "        \n",
    "        # [TODO] Uncomment everything below and understand the data structure:\n",
    "        # The return `samples` is a dict that contains several fields.\n",
    "        # Each field (key-value pair) contains a list.\n",
    "        # Each element in list is a list represent the data in one trajectory (episode).\n",
    "        # Each episode list contains the data of that field of all time steps in that episode.\n",
    "        # The return `sample_info` is a dict contains logging item name and its value.\n",
    "        \n",
    "        samples = {\n",
    "            \"obs\": episode_obs_list,\n",
    "            \"act\": episode_act_list,\n",
    "            \"reward\": episode_reward_list\n",
    "        }\n",
    "        \n",
    "        #pass\n",
    "        \n",
    "        sample_info = {\n",
    "            \"iter_timesteps\": iter_timesteps,\n",
    "            \"iter_episodes\": iter_episodes,\n",
    "            \"performance\": np.mean(episode_rewards),  # help drawing figures\n",
    "            \"training_episode_length\": {\n",
    "                \"mean\": float(np.mean(episode_lens)),\n",
    "                \"max\": float(np.max(episode_lens)),\n",
    "                \"min\": float(np.min(episode_lens))\n",
    "            },\n",
    "            \"training_episode_reward\": {\n",
    "                \"mean\": float(np.mean(episode_rewards)),\n",
    "                \"max\": float(np.max(episode_rewards)),\n",
    "                \"min\": float(np.min(episode_rewards))\n",
    "            }\n",
    "        }\n",
    "        return samples, sample_info\n",
    "\n",
    "    def process_samples(self, samples):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def update_model(self, processed_samples):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    # ===== Training iteration =====\n",
    "    def train(self):\n",
    "        \"\"\"Here we defined the training pipeline using the abstract\n",
    "        functions.\"\"\"\n",
    "        info = dict(iteration=self.iteration)\n",
    "\n",
    "        # [TODO] Uncomment the following block and go through the learning \n",
    "        # pipeline.\n",
    "        \n",
    "        # Collect samples\n",
    "        samples, sample_info = self.collect_samples()\n",
    "        info.update(sample_info)\n",
    "\n",
    "        # Process samples\n",
    "        processed_samples, processed_info = self.process_samples(samples)\n",
    "        info.update(processed_info)\n",
    "\n",
    "        # Update the model\n",
    "        update_info = self.update_model(processed_samples)\n",
    "        info.update(update_info)\n",
    "        \n",
    "        #pass\n",
    "\n",
    "        now = time.time()\n",
    "        self.iteration += 1\n",
    "        self.total_timesteps += info[\"iter_timesteps\"]\n",
    "        self.total_episodes += info[\"iter_episodes\"]\n",
    "        info[\"iter_time\"] = now - self.iteration_time\n",
    "        info[\"total_time\"] = now - self.start_time\n",
    "        info[\"total_episodes\"] = self.total_episodes\n",
    "        info[\"total_timesteps\"] = self.total_timesteps\n",
    "        self.iteration_time = now\n",
    "        return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed!\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "class TestAbstractTrainer(AbstractTrainer):\n",
    "    def build_model(self):\n",
    "        pass\n",
    "    def compute_action(self, _):\n",
    "        return np.random.choice(self.act_dim)\n",
    "test_trainer = TestAbstractTrainer()\n",
    "s, s_info = test_trainer.collect_samples()\n",
    "assert len(s[\"obs\"]) == s_info[\"iter_episodes\"]\n",
    "for i in range(len(s[\"obs\"])):\n",
    "    assert s[\"obs\"][i].shape[0] <= default_config[\"max_episode_length\"]\n",
    "    assert len(s[\"act\"][i]) == len(s[\"obs\"][i]) == len(s[\"reward\"][i])\n",
    "assert abs(s_info[\"training_episode_length\"][\"mean\"] * s_info[\"iter_episodes\"]\n",
    "           - s_info[\"iter_timesteps\"]) < 1\n",
    "print(\"Test Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an abstract trainer class here. We have abstracted the learning process into three phases: Collect samples, process samples and update the model. In the following sections, we will continue to fulfill the blank functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Basic policy gradient method\n",
    "\n",
    "(40 / 100 points)\n",
    "\n",
    "Unlike supervised learning, in RL setting the signal: reward and the output of neural network: the action distribution parameter (like the logits in discrete case or the mean in continous case) have no direct connection in terms of gradient. That is, you can't compute the gradient from the environment to your network output. \n",
    "\n",
    "To leverage the powerful tool of gradient descent, scientists innovated a technique called policy gradient, which is computed on top of the rewards and policy network's output. Scientists prove that: the policy gradient is the gradient of expected return w.r.t. the output. So that by applying gradient descent to gradient descent algorithm you can optimize your policy network and maximize the expected return (or minimize the negative of expected return).\n",
    "\n",
    "Concretely, if we consider the expected return, the value we want to maximize, is:\n",
    "\n",
    "$$Q = \\mathbb E_{\\text{possible trajectories}} \\sum_t r(a_t, s_t) = \\sum_{s_0, a_0,..} p(s_0, a_0, ..., s_t, a_t) r(s_0, a_0, ..., s_t, a_t) = \\sum_{\\tau} p(\\tau)r(\\tau)$$ \n",
    "\n",
    "wherein $\\sum_t r(a_t, s_t) = r(s_0, a_0, ..., s_t, a_t) = r(\\tau)$ is the trajectory return. We use $\\tau$ to represent a trajectory $s_0, a_0, ..., s_t, a_t$. Note that we remove the discount factor for simplicity.\n",
    "\n",
    "Since we want to maximize Q, we can simply compute the gradient from Q to parameter $\\theta$ (which is included in the action probability $p(a_t)$):\n",
    "\n",
    "$$\\nabla_\\theta Q = \\nabla_\\theta \\sum_{\\tau} p(\\tau)r(\\tau) = \\sum_{\\tau} r(\\tau) \\nabla_\\theta p(\\tau)$$\n",
    "\n",
    "Scientists use a famous trick to deal with the gradient of probability of trajectory: $\\nabla_\\theta p(\\tau) = p(\\tau)\\cfrac{\\nabla_\\theta p(\\tau)}{p(\\tau)} = p(\\tau)\\nabla_\\theta \\log p(\\tau)$. \n",
    "\n",
    "The reason for this trick is that the probability of a trajectory is the product of all related probabilities of states and actions. So introducing a log term can change the production to summation: $p_\\theta(\\tau) = p(s_0, a_0, ...) = p(s_0) \\prod_t \\pi_\\theta (a_t|s_t) p(s_{t+1}|s_t, a_t)$ Now we can expand the log of product above to sum of log:\n",
    "\n",
    "$$\\log p_\\theta (\\tau) = \\log p(s_1) + \\sum_t \\log \\pi_\\theta(a_t|s_t) + \\sum_t \\log p(s_{t+1}|s_t, a_t)$$\n",
    "\n",
    "You will find that the first and third term are not related to the parameter of policy $\\pi_\\theta(\\cdot)$. So when we back to $\\nabla_\\theta Q$, we find \n",
    "\n",
    "$$\\nabla_\\theta Q = \\sum_{\\tau} r(\\tau) \\nabla_\\theta p(\\tau) =  \\sum p_\\theta(\\tau) ( \\sum_t  \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) ) r(\\tau) d\\tau$$\n",
    "\n",
    "The probability of trajectory is included when we sampled sufficient data from environment (Yes, so REINFORCE is a Monte Carlo algorithm) so the final form of policy gradient is:\n",
    "\n",
    "$$\\nabla_\\theta Q =\\cfrac{1}{N}\\sum_{i=1}^N [( \\sum_t  \\nabla_\\theta \\log \\pi_\\theta(a_{i,t}|s_{i,t}) (\\sum_{t'=t} \\gamma^{t'-t} r(s_{i,t'}, a_{i,t'}) )]$$\n",
    "\n",
    "This algorithm is called REINFORCE algorithm, which is a Monte Carlo Policy Gradient algorithm having long history. In this section, we will implement the it using pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy network is composed by two parts: a basic MLP serve as function approximator which predict the probabilities of actions given the observation; and a distribution layer building upon the MLP to wrap the raw logits output of neural network to a distribution and provides API for action sampling and log probability retrieving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_units=128):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc0 = nn.Linear(obs_dim, hidden_units)\n",
    "        self.fc1 = nn.Linear(hidden_units, act_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.type_as(self.fc0.bias)\n",
    "        x = F.relu(self.fc0(x))\n",
    "        x = self.fc1(x)\n",
    "        x= F.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CategoricalPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_units=128):\n",
    "        super(CategoricalPolicy, self).__init__()\n",
    "        self.network = Net(obs_dim, act_dim, hidden_units)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        logit = self.network(obs)\n",
    "        \n",
    "        # [TODO] Create an object of the class \"Categorical\" in torch \n",
    "        # package \"distributions\". Then sample an action from it.\n",
    "        action = None\n",
    "        p = torch.distributions.Categorical(logit)\n",
    "        action = p.sample()\n",
    "        #pass\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def log_prob(self, obs, act):\n",
    "        logits = self.network(obs)\n",
    "        \n",
    "        # [TODO] Create an object of the class \"Categorical\" in torch \n",
    "        # package \"distributions\". Then collect the log probability of\n",
    "        # the action in this distribution.\n",
    "        log_prob = None\n",
    "        p = torch.distributions.Categorical(logits)\n",
    "        log_prob = p.log_prob(act)\n",
    "        #pass\n",
    "        \n",
    "        return log_prob\n",
    "\n",
    "# Not that we do not implement GaussianPolicy here. So we can't\n",
    "# apply our algorithm to the environment with continous action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "pg_default_config = merge_config(dict(\n",
    "    normalize_advantage=True,\n",
    "    clip_gradient=10.0,\n",
    "    hidden_units=64,\n",
    "), default_config)\n",
    "\n",
    "\n",
    "class PGTrainer(AbstractTrainer):\n",
    "    def __init__(self, config=None):\n",
    "        config = check_and_merge_config(config or {}, pg_default_config)\n",
    "        super().__init__(config)\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Build the policy network and related optimizer\"\"\"\n",
    "        # Detect whether you have GPU or not. Remember to call X.to(self.device)\n",
    "        # if necessary.\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        \n",
    "        # [TODO] Build the policy network using CategoricalPolicy\n",
    "        # Hint: Remember to pass config[\"hidden_units\"], and set policy network\n",
    "        #  to the device you are using.\n",
    "        self.policy = None\n",
    "        self.policy = CategoricalPolicy(self.obs_dim, self.act_dim,self.config[\"hidden_units\"]).to(self.device)\n",
    "        #pass\n",
    "        \n",
    "        # Build the Adam optimizer.\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.policy.parameters(),\n",
    "            lr=self.config[\"learning_rate\"]\n",
    "        )\n",
    "\n",
    "    def to_tensor(self, array):\n",
    "        \"\"\"Transform a numpy array to a pytorch tensor\"\"\"\n",
    "        return torch.from_numpy(array).to(self.device)\n",
    "\n",
    "    def to_array(self, tensor):\n",
    "        \"\"\"Transform a pytorch tensor to a numpy array\"\"\"\n",
    "        return tensor.cpu().detach().numpy()\n",
    "\n",
    "    def compute_action(self, observation):\n",
    "        \"\"\"Compute the action for single observation\"\"\"\n",
    "        assert observation.ndim == 1\n",
    "        # [TODO] Sample an action from action distribution given by the policy\n",
    "        # Hint: The input of policy network is a batch of data, so you need to\n",
    "        #  expand the first dimension of observation before feeding it to policy network.\n",
    "        action = None\n",
    "        observation = self.to_tensor(observation)\n",
    "        observation = observation.view(1,-1)\n",
    "        action = self.policy(observation)\n",
    "        return action.item()\n",
    "        #pass\n",
    "\n",
    "    def compute_log_probs(self, observation, action):\n",
    "        \"\"\"Compute the log probabilities of a batch of state-action pair\"\"\"\n",
    "        # [TODO] Using the function of policy network to get log probs.\n",
    "        # Hint: Remember to transform the data into tensor before feeding it.\n",
    "        observation = self.to_tensor(observation)\n",
    "        action = self.to_tensor(action)\n",
    "        log_probs =  self.policy.log_prob(observation,action)\n",
    "        return log_probs\n",
    "        #pass\n",
    "\n",
    "    def process_samples(self, samples):\n",
    "        \"\"\"Process samples and add advantages in it\"\"\"\n",
    "        values = []\n",
    "        for reward_list in samples[\"reward\"]:\n",
    "            # reward_list contains rewards in one episode\n",
    "            returns = np.zeros_like(reward_list, dtype=np.float32)\n",
    "            Q = 0\n",
    "            \n",
    "            # [TODO] Scan the episode in a reverse order and compute the\n",
    "            # discounted return at each time step. Fill the array `returns`\n",
    "            # Hint: returns[n-1] = r_n-1\n",
    "            #       returns[n-2] = r_n-2 + gamma * r_n-1\n",
    "            #       ...\n",
    "            #       returns[0] = r_0 + gamma * r_1 + gamma^2 * r_2 + ...\n",
    "            #  wherein n is len(reward_list)\n",
    "            for i,reward in enumerate(reversed(reward_list)):\n",
    "                Q = reward+self.config[\"gamma\"]*Q\n",
    "                returns[-1-i] = Q\n",
    "            #pass\n",
    "            values.append(returns)\n",
    "\n",
    "        # We call the values advantage here.\n",
    "        advantages = np.concatenate(values)\n",
    "\n",
    "        if self.config[\"normalize_advantage\"]:\n",
    "            # [TODO] normalize the advantage so that it's mean is\n",
    "            # almost 0 and the its standard deviation is almost 1.\n",
    "            advantages -= np.mean(advantages)\n",
    "            advantages /= np.std(advantages)\n",
    "            #pass\n",
    "        \n",
    "        samples[\"advantages\"] = advantages\n",
    "        return samples, {}\n",
    "\n",
    "    def update_model(self, processed_samples):\n",
    "        \"\"\"A wrapper function, call self.update_policy\"\"\"\n",
    "        return self.update_policy(processed_samples)\n",
    "\n",
    "    def update_policy(self, processed_samples):\n",
    "        \"\"\"Update the policy network\"\"\"\n",
    "        advantages = self.to_tensor(processed_samples[\"advantages\"])\n",
    "        flat_obs = np.concatenate(processed_samples[\"obs\"])\n",
    "        flat_act = np.concatenate(processed_samples[\"act\"])\n",
    "\n",
    "        self.policy.train()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        log_probs = self.compute_log_probs(flat_obs, flat_act)\n",
    "\n",
    "        assert log_probs.shape == advantages.shape, \"log_probs shape {} is not \" \\\n",
    "            \"compatible with advantages {}\".format(log_probs.shape, advantages.shape)\n",
    "        \n",
    "        # [TODO] Compute the loss using log probabilities and advantages.\n",
    "        loss = None\n",
    "        loss = -(log_probs*advantages).sum() \n",
    "        #pass\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip the gradient\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            self.policy.parameters(), self.config[\"clip_gradient\"]\n",
    "        )\n",
    "\n",
    "        self.optimizer.step()\n",
    "        self.policy.eval()\n",
    "        update_info = {\n",
    "            \"policy_loss\": loss.item(),\n",
    "            \"mean_log_prob\": torch.mean(log_probs).item(),\n",
    "            \"mean_advantage\": torch.mean(advantages).item()\n",
    "        }\n",
    "        return update_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\ierg6130\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Passed!\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# Test advantage computing\n",
    "test_trainer = PGTrainer({\"normalize_advantage\": False})\n",
    "test_trainer.train()\n",
    "fake_sample = {\"reward\": [[2, 2, 2, 2, 2]]}\n",
    "assert_almost_equal(\n",
    "    test_trainer.process_samples(fake_sample)[0][\"reward\"][0],\n",
    "    fake_sample[\"reward\"][0]\n",
    ")\n",
    "assert_almost_equal(\n",
    "    test_trainer.process_samples(fake_sample)[0][\"advantages\"],\n",
    "    np.array([9.80199, 7.880798, 5.9402, 3.98, 2.], dtype=np.float32)\n",
    ")\n",
    "\n",
    "# Test advantage normalization\n",
    "test_trainer = PGTrainer(\n",
    "    {\"normalize_advantage\": True, \"env_name\": \"LunarLander-v2\"})\n",
    "test_adv = test_trainer.process_samples(fake_sample)[0][\"advantages\"]\n",
    "assert_almost_equal(test_adv.mean(), 0.0)\n",
    "assert_almost_equal(test_adv.std(), 1.0)\n",
    "\n",
    "# Test the shape of functions' returns\n",
    "fake_observation = np.array([\n",
    "    test_trainer.env.observation_space.sample() for i in range(10)\n",
    "])\n",
    "fake_action = np.array([\n",
    "    test_trainer.env.action_space.sample() for i in range(10)\n",
    "])\n",
    "assert test_trainer.to_tensor(fake_observation).shape == torch.Size([10, 8])\n",
    "assert np.array(test_trainer.compute_action(fake_observation[0])).shape == ()\n",
    "assert test_trainer.compute_log_probs(fake_observation, fake_action).shape == \\\n",
    "       torch.Size([10])\n",
    "\n",
    "print(\"Test Passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PGTrainer CartPole-v0 Training Start ===\n",
      "Config:\n",
      "  checked: true\n",
      "  clip_gradient: 10.0\n",
      "  env_name: CartPole-v0\n",
      "  evaluate_interval: 10\n",
      "  evaluate_num_episodes: 50\n",
      "  gamma: 0.99\n",
      "  hidden_units: 64\n",
      "  learning_rate: 0.01\n",
      "  max_episode_length: 200\n",
      "  max_iteration: 1000\n",
      "  normalize_advantage: false\n",
      "  seed: 0\n",
      "  train_batch_size: 200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\ierg6130\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PGTrainer CartPole-v0 Iteration 0 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 23.18\n",
      "  iter_episodes: 10\n",
      "  iter_time: 0.11746907234191895\n",
      "  iter_timesteps: 217\n",
      "  iteration: 0\n",
      "  mean_advantage: 11.939602851867676\n",
      "  mean_log_prob: -0.6919979453086853\n",
      "  performance: 21.7\n",
      "  policy_loss: 1793.763427734375\n",
      "  total_episodes: 10\n",
      "  total_time: 0.11746907234191895\n",
      "  total_timesteps: 217\n",
      "  training_episode_length:\n",
      "    max: 32.0\n",
      "    mean: 21.7\n",
      "    min: 9.0\n",
      "  training_episode_reward:\n",
      "    max: 32.0\n",
      "    mean: 21.7\n",
      "    min: 9.0\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 10 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 58.24\n",
      "  iter_episodes: 3\n",
      "  iter_time: 0.11713433265686035\n",
      "  iter_timesteps: 246\n",
      "  iteration: 10\n",
      "  mean_advantage: 34.35287094116211\n",
      "  mean_log_prob: -0.6174345016479492\n",
      "  performance: 82.0\n",
      "  policy_loss: 5194.11328125\n",
      "  total_episodes: 63\n",
      "  total_time: 1.9001731872558594\n",
      "  total_timesteps: 2549\n",
      "  training_episode_length:\n",
      "    max: 123.0\n",
      "    mean: 82.0\n",
      "    min: 59.0\n",
      "  training_episode_reward:\n",
      "    max: 123.0\n",
      "    mean: 82.0\n",
      "    min: 59.0\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 20 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 100.46\n",
      "  iter_episodes: 3\n",
      "  iter_time: 0.11999940872192383\n",
      "  iter_timesteps: 245\n",
      "  iteration: 20\n",
      "  mean_advantage: 32.579036712646484\n",
      "  mean_log_prob: -0.6084087491035461\n",
      "  performance: 81.66666666666667\n",
      "  policy_loss: 4873.60595703125\n",
      "  total_episodes: 96\n",
      "  total_time: 4.521067142486572\n",
      "  total_timesteps: 5005\n",
      "  training_episode_length:\n",
      "    max: 94.0\n",
      "    mean: 81.66666666666667\n",
      "    min: 64.0\n",
      "  training_episode_reward:\n",
      "    max: 94.0\n",
      "    mean: 81.66666666666667\n",
      "    min: 64.0\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 30 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 138.96\n",
      "  iter_episodes: 3\n",
      "  iter_time: 0.12503552436828613\n",
      "  iter_timesteps: 272\n",
      "  iteration: 30\n",
      "  mean_advantage: 46.58760452270508\n",
      "  mean_log_prob: -0.6039609313011169\n",
      "  performance: 90.66666666666667\n",
      "  policy_loss: 7558.833984375\n",
      "  total_episodes: 120\n",
      "  total_time: 8.41395616531372\n",
      "  total_timesteps: 7715\n",
      "  training_episode_length:\n",
      "    max: 200.0\n",
      "    mean: 90.66666666666667\n",
      "    min: 27.0\n",
      "  training_episode_reward:\n",
      "    max: 200.0\n",
      "    mean: 90.66666666666667\n",
      "    min: 27.0\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 40 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 175.12\n",
      "  iter_episodes: 2\n",
      "  iter_time: 0.17737817764282227\n",
      "  iter_timesteps: 371\n",
      "  iteration: 40\n",
      "  mean_advantage: 54.99089813232422\n",
      "  mean_log_prob: -0.5928562879562378\n",
      "  performance: 185.5\n",
      "  policy_loss: 12080.87890625\n",
      "  total_episodes: 140\n",
      "  total_time: 13.455306768417358\n",
      "  total_timesteps: 10924\n",
      "  training_episode_length:\n",
      "    max: 200.0\n",
      "    mean: 185.5\n",
      "    min: 171.0\n",
      "  training_episode_reward:\n",
      "    max: 200.0\n",
      "    mean: 185.5\n",
      "    min: 171.0\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 50 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 161.72\n",
      "  iter_episodes: 2\n",
      "  iter_time: 0.17296552658081055\n",
      "  iter_timesteps: 396\n",
      "  iteration: 50\n",
      "  mean_advantage: 56.83638000488281\n",
      "  mean_log_prob: -0.5828351378440857\n",
      "  performance: 198.0\n",
      "  policy_loss: 13076.001953125\n",
      "  total_episodes: 161\n",
      "  total_time: 19.35023021697998\n",
      "  total_timesteps: 14241\n",
      "  training_episode_length:\n",
      "    max: 200.0\n",
      "    mean: 198.0\n",
      "    min: 196.0\n",
      "  training_episode_reward:\n",
      "    max: 200.0\n",
      "    mean: 198.0\n",
      "    min: 196.0\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 60 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 173.6\n",
      "  iter_episodes: 2\n",
      "  iter_time: 0.18755841255187988\n",
      "  iter_timesteps: 400\n",
      "  iteration: 60\n",
      "  mean_advantage: 57.131980895996094\n",
      "  mean_log_prob: -0.5676707625389099\n",
      "  performance: 200.0\n",
      "  policy_loss: 13028.9755859375\n",
      "  total_episodes: 182\n",
      "  total_time: 24.080311059951782\n",
      "  total_timesteps: 17720\n",
      "  training_episode_length:\n",
      "    max: 200.0\n",
      "    mean: 200.0\n",
      "    min: 200.0\n",
      "  training_episode_reward:\n",
      "    max: 200.0\n",
      "    mean: 200.0\n",
      "    min: 200.0\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 70 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 195.84\n",
      "  iter_episodes: 2\n",
      "  iter_time: 0.17192435264587402\n",
      "  iter_timesteps: 400\n",
      "  iteration: 70\n",
      "  mean_advantage: 57.131980895996094\n",
      "  mean_log_prob: -0.5589452981948853\n",
      "  performance: 200.0\n",
      "  policy_loss: 12739.451171875\n",
      "  total_episodes: 202\n",
      "  total_time: 29.791563272476196\n",
      "  total_timesteps: 21213\n",
      "  training_episode_length:\n",
      "    max: 200.0\n",
      "    mean: 200.0\n",
      "    min: 200.0\n",
      "  training_episode_reward:\n",
      "    max: 200.0\n",
      "    mean: 200.0\n",
      "    min: 200.0\n",
      "\n",
      "In 70 iteration, current mean episode reward 195.840 is greater than reward threshold 195.0. Congratulation! Now we exit the training process.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "pg_trainer_no_na, pg_result_no_na = run(PGTrainer, dict(\n",
    "    learning_rate=0.01,\n",
    "    max_episode_length=200,\n",
    "    train_batch_size=200,\n",
    "    normalize_advantage=False,  # <<== Here!\n",
    "), 195.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PGTrainer CartPole-v0 Training Start ===\n",
      "Config:\n",
      "  checked: true\n",
      "  clip_gradient: 10.0\n",
      "  env_name: CartPole-v0\n",
      "  evaluate_interval: 10\n",
      "  evaluate_num_episodes: 50\n",
      "  gamma: 0.99\n",
      "  hidden_units: 64\n",
      "  learning_rate: 0.01\n",
      "  max_episode_length: 200\n",
      "  max_iteration: 1000\n",
      "  normalize_advantage: true\n",
      "  seed: 0\n",
      "  train_batch_size: 200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\ierg6130\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PGTrainer CartPole-v0 Iteration 0 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 27.24\n",
      "  iter_episodes: 10\n",
      "  iter_time: 0.0750279426574707\n",
      "  iter_timesteps: 217\n",
      "  iteration: 0\n",
      "  mean_advantage: -1.5381843354589364e-08\n",
      "  mean_log_prob: -0.6919979453086853\n",
      "  performance: 21.7\n",
      "  policy_loss: 0.11937487125396729\n",
      "  total_episodes: 10\n",
      "  total_time: 0.0750279426574707\n",
      "  total_timesteps: 217\n",
      "  training_episode_length:\n",
      "    max: 32.0\n",
      "    mean: 21.7\n",
      "    min: 9.0\n",
      "  training_episode_reward:\n",
      "    max: 32.0\n",
      "    mean: 21.7\n",
      "    min: 9.0\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 10 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 67.16\n",
      "  iter_episodes: 2\n",
      "  iter_time: 0.0937807559967041\n",
      "  iter_timesteps: 248\n",
      "  iteration: 10\n",
      "  mean_advantage: -1.509343405814434e-07\n",
      "  mean_log_prob: -0.6186388731002808\n",
      "  performance: 124.0\n",
      "  policy_loss: -0.6669702529907227\n",
      "  total_episodes: 67\n",
      "  total_time: 1.6582772731781006\n",
      "  total_timesteps: 2526\n",
      "  training_episode_length:\n",
      "    max: 169.0\n",
      "    mean: 124.0\n",
      "    min: 79.0\n",
      "  training_episode_reward:\n",
      "    max: 169.0\n",
      "    mean: 124.0\n",
      "    min: 79.0\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 20 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 123.32\n",
      "  iter_episodes: 3\n",
      "  iter_time: 0.10100650787353516\n",
      "  iter_timesteps: 244\n",
      "  iteration: 20\n",
      "  mean_advantage: 1.9444793508682778e-07\n",
      "  mean_log_prob: -0.6076633334159851\n",
      "  performance: 81.33333333333333\n",
      "  policy_loss: 2.99189829826355\n",
      "  total_episodes: 100\n",
      "  total_time: 4.2097461223602295\n",
      "  total_timesteps: 5084\n",
      "  training_episode_length:\n",
      "    max: 115.0\n",
      "    mean: 81.33333333333333\n",
      "    min: 64.0\n",
      "  training_episode_reward:\n",
      "    max: 115.0\n",
      "    mean: 81.33333333333333\n",
      "    min: 64.0\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 30 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 173.5\n",
      "  iter_episodes: 2\n",
      "  iter_time: 0.17080211639404297\n",
      "  iter_timesteps: 385\n",
      "  iteration: 30\n",
      "  mean_advantage: 1.7958801379336364e-07\n",
      "  mean_log_prob: -0.5720041990280151\n",
      "  performance: 192.5\n",
      "  policy_loss: -8.29409408569336\n",
      "  total_episodes: 123\n",
      "  total_time: 8.587901592254639\n",
      "  total_timesteps: 7925\n",
      "  training_episode_length:\n",
      "    max: 200.0\n",
      "    mean: 192.5\n",
      "    min: 185.0\n",
      "  training_episode_reward:\n",
      "    max: 200.0\n",
      "    mean: 192.5\n",
      "    min: 185.0\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 40 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 185.6\n",
      "  iter_episodes: 2\n",
      "  iter_time: 0.1856377124786377\n",
      "  iter_timesteps: 400\n",
      "  iteration: 40\n",
      "  mean_advantage: 6.973743182925318e-08\n",
      "  mean_log_prob: -0.5700274109840393\n",
      "  performance: 200.0\n",
      "  policy_loss: -0.41051149368286133\n",
      "  total_episodes: 143\n",
      "  total_time: 14.233988761901855\n",
      "  total_timesteps: 11736\n",
      "  training_episode_length:\n",
      "    max: 200.0\n",
      "    mean: 200.0\n",
      "    min: 200.0\n",
      "  training_episode_reward:\n",
      "    max: 200.0\n",
      "    mean: 200.0\n",
      "    min: 200.0\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 50 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 173.5\n",
      "  iter_episodes: 2\n",
      "  iter_time: 0.1406705379486084\n",
      "  iter_timesteps: 363\n",
      "  iteration: 50\n",
      "  mean_advantage: 1.563185207942297e-07\n",
      "  mean_log_prob: -0.5274372100830078\n",
      "  performance: 181.5\n",
      "  policy_loss: -9.067272186279297\n",
      "  total_episodes: 163\n",
      "  total_time: 19.461881160736084\n",
      "  total_timesteps: 15117\n",
      "  training_episode_length:\n",
      "    max: 188.0\n",
      "    mean: 181.5\n",
      "    min: 175.0\n",
      "  training_episode_reward:\n",
      "    max: 188.0\n",
      "    mean: 181.5\n",
      "    min: 175.0\n",
      "\n",
      "=== PGTrainer CartPole-v0 Iteration 60 ===\n",
      "Training Progress:\n",
      "  evaluate_reward: 200.0\n",
      "  iter_episodes: 2\n",
      "  iter_time: 0.1562962532043457\n",
      "  iter_timesteps: 400\n",
      "  iteration: 60\n",
      "  mean_advantage: 6.973743182925318e-08\n",
      "  mean_log_prob: -0.49633103609085083\n",
      "  performance: 200.0\n",
      "  policy_loss: -3.438786029815674\n",
      "  total_episodes: 183\n",
      "  total_time: 24.437317848205566\n",
      "  total_timesteps: 19074\n",
      "  training_episode_length:\n",
      "    max: 200.0\n",
      "    mean: 200.0\n",
      "    min: 200.0\n",
      "  training_episode_reward:\n",
      "    max: 200.0\n",
      "    mean: 200.0\n",
      "    min: 200.0\n",
      "\n",
      "In 60 iteration, current mean episode reward 200.000 is greater than reward threshold 195.0. Congratulation! Now we exit the training process.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "pg_trainer_na, pg_result_na = run(PGTrainer, dict(\n",
    "    learning_rate=0.01,\n",
    "    max_episode_length=200,\n",
    "    train_batch_size=200,\n",
    "    normalize_advantage=True,  # <<== Here!\n",
    "), 195.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Policy Gradient')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "pg_result_no_na[\"normalize_advantage\"] = False\n",
    "pg_result_na[\"normalize_advantage\"] = True\n",
    "pg_result = pd.concat([pg_result_no_na, pg_result_na])\n",
    "ax = sns.lineplot(\n",
    "    x=\"total_timesteps\", \n",
    "    y=\"performance\", \n",
    "    data=pg_result, hue=\"normalize_advantage\",\n",
    ")\n",
    "ax.set_title(\"Policy Gradient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Policy gradient with baseline\n",
    "\n",
    "(25 / 100 points)\n",
    "\n",
    "Recall the REINFORCE algorithm above. We compute the gradient of $Q = \\mathop{\\mathbb E} \\sum_t r(a_t, s_t)$ w.r.t. the parameter to update the policy. A natural question is that, when you take an inferior action that lead to positive expected return, the policy gradient is also positive and you will update your network toward this action. At the same time a potential better action is ignored.\n",
    "\n",
    "To tackle this problem, we introduce \"baseline\" when computing the policy gradient. The insight behind this is that we want to optimize the policy toward an action that are better than the average performance when taking all action into consideration at a given state.\n",
    "\n",
    "We introduce $b_{t} = \\mathbb E_{a_t} \\sum_{t'}{\\gamma^{t'-t} r(s_{t'}, a_{t'})}$ as the baseline. It average all possible actions at state $s_t$. So that the profit achieved by action $a_t$ can be evaluated via $\\sum_{t'=t} \\gamma^{t' -t}r(a_{t'}, s_{t'}) - b_t$\n",
    "\n",
    "Therefore, the policy gradient becomes:\n",
    "\n",
    "$$\\nabla_\\theta Q =\\cfrac{1}{N}\\sum_{i=1}^N [( \\sum_t  \\nabla_\\theta \\log \\pi_\\theta(a_{i,t}|s_{i,t}) (\\sum_{t'} \\gamma^{t'-t} r(s_{i,{t}}, a_{i,t}) - b_{i, t})]$$\n",
    "\n",
    "In our implementation, we estimate the baseline via an extra network `self.baseline`, which has same structure of policy network but output only a scalar value. We use the output of this network to serve as the baseline, while this network is updated by fitting the true value of expected return of current state: $\\mathbb E_{a_t} \\sum_{t'}{\\gamma^{t'-t} r(s_{t'}, a_{t'})}$\n",
    "\n",
    "In implementation, we use a trick to match the distribution of baseline and values. During training, we first collect a batch of target values: $\\{t_i= \\mathbb E_{a_t} \\sum_{t'}{\\gamma^{t'-t} r(s_{t'}, a_{t'})}\\}_i$. Then we normalzie all targets to standard distribution with mean = 0 and std = 1. Then we ask the baseline network to fit such normalized targets.\n",
    "\n",
    "When computing the advantages, instead of using the output of baseline network as the baseline $b$, we firstly match the baseline distribution with action values. The transformed baselines $b' = f(b)$ should has the same means of action values and same STD of action values too. Then we compute the advantage of current action: $adv_{i,t} = \\sum_{t'} \\gamma^{t'-t} r(s_{i,{t'}}, a_{i,t'}) - b'_{i, t}$\n",
    "\n",
    "By doing this, we mitigate the instability of training baseline.\n",
    "\n",
    "Hint: We suggest to normalize an array via: `(x - x.mean()) / max(x.std(), 1e-6)`. The max term can avoid potential risk of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_with_baseline_default_config = merge_config(dict(), pg_default_config)\n",
    "\n",
    "class PolicyGradientWithBaselineTrainer(PGTrainer):\n",
    "    def __init__(self, config=None):\n",
    "        config = check_and_merge_config(config or {}, pg_with_baseline_default_config)\n",
    "        super().__init__(config)\n",
    "\n",
    "    def build_model(self):\n",
    "        # Build the actor in name of self.policy\n",
    "        super().build_model()\n",
    "        \n",
    "        # [TODO] Build the baseline network using Net class.\n",
    "        # Remember that you need to set the output dimension to 1.\n",
    "        # Remember using model.to(device) function to move your model to GPU (if applicable)\n",
    "        self.baseline = None\n",
    "        pass\n",
    "        \n",
    "        self.baseline_loss = nn.MSELoss()\n",
    "            \n",
    "        self.baseline_optimizer = torch.optim.Adam(\n",
    "            self.baseline.parameters(),\n",
    "            lr=self.config[\"learning_rate\"]\n",
    "        )\n",
    "\n",
    "    def process_samples(self, samples):\n",
    "        # Call the original process_samples function to get advantages\n",
    "        tmp_samples, _ = super().process_samples(samples)\n",
    "        values = tmp_samples[\"advantages\"]\n",
    "        samples[\"values\"] = values  # We add q_values into samples\n",
    "\n",
    "        # [TODO] flatten the observations in all trajectories (still a numpy array)\n",
    "        obs = None\n",
    "        pass\n",
    "        \n",
    "        assert obs.ndim == 2\n",
    "        assert obs.shape[1] == self.obs_dim\n",
    "        \n",
    "        obs = self.to_tensor(obs)\n",
    "        samples[\"flat_obs\"] = obs\n",
    "\n",
    "        # [TODO] Compute the baseline by feeding observation to baseline network\n",
    "        # Hint: baselines turns out to be a numpy array with the same shape of `values`,\n",
    "        #  that is: (batch size, )\n",
    "        baselines = None\n",
    "        pass\n",
    "        \n",
    "        assert baselines.shape == values.shape\n",
    "        \n",
    "        # [TODO] Match the distribution of baselines to the values.\n",
    "        # Hint: We expect to see baselines.std almost equals to values.std, \n",
    "        #  and baselines.mean almost equals to values.mean\n",
    "        pass\n",
    "\n",
    "        # Compute the advantage\n",
    "        advantages = values - baselines\n",
    "        samples[\"advantages\"] = advantages\n",
    "        process_info = {\"mean_baseline\": float(np.mean(baselines))}\n",
    "        return samples, process_info\n",
    "\n",
    "    def update_model(self, processed_samples):\n",
    "        update_info = {}\n",
    "        update_info.update(self.update_baseline(processed_samples))\n",
    "        update_info.update(self.update_policy(processed_samples))\n",
    "        return update_info\n",
    "\n",
    "    def update_baseline(self, processed_samples):\n",
    "        self.baseline.train()\n",
    "        obs = processed_samples[\"flat_obs\"]\n",
    "\n",
    "        # [TODO] Normalize the values to mean=0, std=1.\n",
    "        values = processed_samples[\"values\"]\n",
    "        pass\n",
    "        \n",
    "        values = self.to_tensor(values[:, np.newaxis])\n",
    "        \n",
    "        baselines = self.baseline(obs)\n",
    "\n",
    "        self.baseline_optimizer.zero_grad()\n",
    "        loss = self.baseline_loss(input=baselines, target=values)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip the gradient\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            self.baseline.parameters(), self.config[\"clip_gradient\"]\n",
    "        )\n",
    "        \n",
    "        self.baseline_optimizer.step()\n",
    "        self.baseline.eval()\n",
    "        return dict(baseline_loss=loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "pg_trainer_wb, pgwb_result_wb = run(PolicyGradientWithBaselineTrainer, dict(\n",
    "    learning_rate=0.01,\n",
    "    max_episode_length=200,\n",
    "    train_batch_size=200,\n",
    "), 195.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "pg_result_no_na[\"normalize_advantage\"] = False\n",
    "pg_result_na[\"normalize_advantage\"] = True\n",
    "pgwb_result_wb[\"normalize_advantage\"] = \"True with Baseline\"\n",
    "pg_result = pd.concat([pg_result_no_na, pg_result_na, pgwb_result_wb])\n",
    "ax = sns.lineplot(\n",
    "    x=\"total_timesteps\", \n",
    "    y=\"performance\", \n",
    "    data=pg_result, hue=\"normalize_advantage\",\n",
    ")\n",
    "ax.set_title(\"Policy Gradient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# You should see a pop up window which display the movement of the cart and pole.\n",
    "print(\"Average episode reward for your Policy Gradient agent in CartPole-v0: \",\n",
    "      pg_trainer_wb.evaluate(1, render=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Actor-critic Method\n",
    "\n",
    "(30 / 100 points totally)\n",
    "\n",
    "### Section 4.1: Implement ActorCriticTrainer\n",
    "\n",
    "(20 / 100 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As point out in the textbook Page 331, the difference of Actor-critic methods and policy gradient with baseline method is whether the critic (baseline) participates in the bootstrapping. In `PolicyGradientWithBaselineTrainer`, we update the baseline network by fitting the ground-truth Q values. However, in actor-critic trainer presented below, we update the critic via fitting a bootstrapped target: \n",
    "\n",
    "$$\\delta = r_t + \\gamma V(s_{t+1}) - V(s_t)$$\n",
    "\n",
    "wherein $V$ is the values predicted by critic. In `ActorCriticTrainer`, in order to reuse the codes, we use `self.baseline` to represent the critic network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "ac_default_config = merge_config(dict(\n",
    "    normalize_advantage=True,\n",
    "    num_critic_updates=10,\n",
    "    num_critic_update_steps=10\n",
    "), pg_with_baseline_default_config)\n",
    "\n",
    "class ActorCriticTrainer(PolicyGradientWithBaselineTrainer):\n",
    "    def __init__(self, config=None):\n",
    "        config = check_and_merge_config(config or {}, ac_default_config)\n",
    "        super().__init__(config)\n",
    "        \n",
    "    def process_samples(self, samples):\n",
    "        \"\"\"\n",
    "        In this function, we will compute the target of value function\n",
    "        and also the advantage used for updating policy.\n",
    "        \n",
    "        First, we need to prepare data to compute the values.\n",
    "        \n",
    "        Consider you only collect two trajectories in samples.\n",
    "        Then the data structure should be:\n",
    "            samples[\"obs\"] = [\n",
    "                [\n",
    "                    obs_t1_0,  <<= t=0\n",
    "                    obs_t1_1,  <<= t=1\n",
    "                    obs_t1_n1-1, <<= t=n1-1 \n",
    "                ],  (the traj t1 has length = n1)\n",
    "                [\n",
    "                    obs_t2_0,\n",
    "                    ...\n",
    "                    obs_t2_n2-1 \n",
    "                ]   (the traj t2 has length = n2)\n",
    "            ]\n",
    "        the whole training batch size is N = (n1 + n2).\n",
    "        Then what you should do is to take store\n",
    "        [\n",
    "            obs_t1_1, ..., obs_t1_n1-1, ZERO, \n",
    "            obs_t2_1, ... obs_t2_n2-1, ZERO\n",
    "        ]\n",
    "        into `next_obs` variable, wherein ZERO is a zero array has\n",
    "        same shape as any other observation.\n",
    "        \n",
    "        Besides, you also need to prepare a boolean array `masks`\n",
    "        who has length N. It should be like:\n",
    "        [\n",
    "            False(0), ..., False(n1-2), True(n1-1),\n",
    "            False(0), ..., False(n2-2), True(n2-1)\n",
    "        ].\n",
    "        It represents whether a transition is terminal transition\n",
    "        and whether a observation in next_obs should be masked.\n",
    "        We will used it when computing the bootstrapped values:\n",
    "        \n",
    "            value_t = reward_t + gamma * value_t+1 * (1 - masks)\n",
    "            \n",
    "        So that we will not consider the impact of value_t+1\n",
    "        in terminal state.\n",
    "        \n",
    "        Then, we need to compute the values and advantage based on \n",
    "        the data collected and form a processed_samples for the \n",
    "        consecetive .\n",
    "        \"\"\"\n",
    "        # Define the sum of all trajectory length as N (train batch size)\n",
    "        N = sum([len(traj) for traj in samples[\"obs\"]])\n",
    "        n = len(samples[\"obs\"][0])  # the first traj length\n",
    "        \n",
    "        # [TODO] Create an array named next_obs.\n",
    "        # In each trajectory, take the observations from second\n",
    "        # transitions to the end. Fill an extra all zero observation\n",
    "        # at the end of trajectory to align next_obs with others.\n",
    "        next_obs = []\n",
    "        for obs_list in samples[\"obs\"]:\n",
    "            pass\n",
    "            \n",
    "        next_obs = np.array(next_obs)\n",
    "        \n",
    "        assert next_obs.shape == (N, self.obs_dim)\n",
    "        assert np.all(next_obs[n - 1] == 0.0)\n",
    "        \n",
    "        # [TODO] Scan all trajectories and create a boolean array `masks`.\n",
    "        # It should be N length, N is the sum of the trajectories lengths.\n",
    "        # You should loop over each trajectory and store n-1 False into masks\n",
    "        # while storing one True at the end of each episode. n is the length\n",
    "        # of current trajectory.\n",
    "        # Finally, you should transform the list to an float32 array\n",
    "        # with shape (N,)\n",
    "        masks = []\n",
    "        for obs_list in samples[\"obs\"]:\n",
    "            pass\n",
    "            \n",
    "        masks = np.array(masks, dtype=np.float32)\n",
    "        \n",
    "        assert masks.shape == (N,)\n",
    "        assert masks[n - 1] == True\n",
    "        \n",
    "        # flatten rewards\n",
    "        rewards = np.concatenate(samples[\"reward\"])\n",
    "        tensor_rewards = self.to_tensor(rewards)\n",
    "        \n",
    "        # flatten observations\n",
    "        obs = np.concatenate(samples[\"obs\"])\n",
    "        \n",
    "        # [TODO] Compute the bootstrapped values\n",
    "        # Hint: value_t = reward_t + gamma * value_t+1 * (1 - masks)\n",
    "        #  You need to ask self.baseline using next_obs to get \n",
    "        #  the values in next state, then can you compute the \n",
    "        #  value_t.\n",
    "        tensor_next_obs = self.to_tensor(next_obs)\n",
    "        values = None\n",
    "        pass\n",
    "        \n",
    "        assert isinstance(values, np.ndarray)\n",
    "        assert values.shape == (N,)\n",
    "        \n",
    "        # [TODO] Compute the baseline by feeding obs to critic\n",
    "        tensor_obs = self.to_tensor(obs)\n",
    "        baselines = None\n",
    "        pass\n",
    "        \n",
    "        assert isinstance(baselines, np.ndarray)\n",
    "        assert baselines.shape == (N,)\n",
    "        \n",
    "        # [TODO] Compute the advantages using values and baselines\n",
    "        advantages = None\n",
    "        pass\n",
    "        \n",
    "        if self.config[\"normalize_advantage\"]:\n",
    "            # [TODO] normalize the advantages\n",
    "            pass\n",
    "        \n",
    "        # We passed part of numpy array and part of torch tensor\n",
    "        # to the following modules.\n",
    "        processed_samples = samples\n",
    "        processed_samples[\"advantages\"] = advantages\n",
    "        processed_samples[\"tensor_next_obs\"] = tensor_next_obs\n",
    "        processed_samples[\"tensor_obs\"] = tensor_obs\n",
    "        processed_samples[\"tensor_rewards\"] = tensor_rewards\n",
    "        processed_samples[\"tensor_masks\"] = self.to_tensor(masks)\n",
    "        process_info = {\n",
    "            \"mean_baselines\": float(np.mean(baselines)),\n",
    "            \"advantages_std\": float(advantages.std())\n",
    "        }  # You can add value here so that it will be printed automatically.\n",
    "        return processed_samples, process_info\n",
    "        \n",
    "    def update_baseline(self, processed_samples):\n",
    "        # Use a bootstrapped target values and update the critic.\n",
    "        # Compute the target values r(s, a) + gamma * V(s') by calling\n",
    "        # the critic to compute V(s').\n",
    "        reward = processed_samples[\"tensor_rewards\"]\n",
    "        obs = processed_samples[\"tensor_obs\"]\n",
    "        next_obs = processed_samples[\"tensor_next_obs\"]\n",
    "        masks = processed_samples[\"tensor_masks\"]\n",
    "        \n",
    "        assert masks.shape == reward.shape\n",
    "        assert next_obs.shape == obs.shape\n",
    "        \n",
    "        losses = []\n",
    "        \n",
    "        for _ in range(self.config[\"num_critic_updates\"]):  # Train critic for 10 iteration\n",
    "            self.baseline.eval()\n",
    "            \n",
    "            # Predict the values of next state\n",
    "            values_next = self.baseline(next_obs).reshape(reward.shape)  # a tensor\n",
    "            \n",
    "            assert values_next.shape == reward.shape  # tensor shape should equal\n",
    "            \n",
    "            # [TODO] Compute the target of critic using reward, masks\n",
    "            # and values_next (don't forget gamma)\n",
    "            # Hint: the masks are all False (that is, zero) when the state is not\n",
    "            #  terminal state, and are True (one) when is. So we can use (1-masks)\n",
    "            #  as a multiplier to apply to values_next when computing the target.\n",
    "            target = None\n",
    "            pass\n",
    "            \n",
    "            target = target.detach()\n",
    "            self.baseline.train()\n",
    "\n",
    "            for _ in range(self.config[\"num_critic_update_steps\"]):\n",
    "                # [TODO] Uncomment the whole section\n",
    "                \"\"\"\n",
    "                baselines = self.baseline(obs).reshape(reward.shape)\n",
    "                loss = self.baseline_loss(input=baselines, target=target)\n",
    "                self.baseline_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.baseline.parameters(), self.config[\"clip_gradient\"]\n",
    "                )\n",
    "                self.baseline_optimizer.step()\n",
    "                losses.append(loss.item())\n",
    "                \"\"\"\n",
    "                pass\n",
    "            \n",
    "        self.baseline.eval()\n",
    "        return {\"critic_loss\": np.mean(losses)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "ac_trainer, ac_result = run(ActorCriticTrainer, dict(\n",
    "    learning_rate=0.01,\n",
    "    max_episode_length=200,\n",
    "    train_batch_size=200,\n",
    "), 195.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "pg_result_na[\"label\"] = \"Pure PG\"\n",
    "pgwb_result_wb[\"label\"] = \"PG with Baseline\"\n",
    "ac_result[\"label\"] = \"Actor-critic\"\n",
    "pg_result = pd.concat([pg_result_na, pgwb_result_wb, ac_result])\n",
    "ax = sns.lineplot(\n",
    "    x=\"total_timesteps\", \n",
    "    y=\"performance\", \n",
    "    data=pg_result, hue=\"label\",\n",
    ")\n",
    "ax.set_title(\"Policy Gradient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4.2 -- Land on the Moon!\n",
    "\n",
    "(5 / 100 points)\n",
    "\n",
    "Let's try to land on the moon! In this section, we will try a harder environment: \"LunarLander-v2\". The agent needs to control the lander to land on a small interval of the moon marked by the flags. Due to the complex dynamic of rocket, the environment is hard to solve and it may take hours for you to train. \n",
    "\n",
    "The environment provides a 8-element vector as observation, which is different from the image observation provided by the following Pong environment in the next setion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "e = gym.make(\"LunarLander-v2\"); e.reset()\n",
    "frames = []\n",
    "for _ in range(1000):\n",
    "    frames.append(e.render(\"rgb_array\"))\n",
    "    if e.step(e.action_space.sample())[2]: break\n",
    "e.close();animate(frames);del e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "ac_trainer_lander, ac_result_lander = run(ActorCriticTrainer, dict(\n",
    "    env_name=\"LunarLander-v2\",\n",
    "    max_iteration=500,\n",
    "    learning_rate=0.005,\n",
    "    max_episode_length=1000,\n",
    "    train_batch_size=10000,\n",
    "    normalize_advantage=True,\n",
    "    evaluate_interval=1,\n",
    "    evaluate_num_episodes=5,\n",
    "), 200)\n",
    "\n",
    "# Hint: 1. This would take hours to train on personal laptop.\n",
    "#       2. Episode reward should greater than 0 after 80 iterations.\n",
    "#       3. We are using purely on-policy algorithm, so we expect the performance \n",
    "#          to be highly unstable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# Plot the result to diagnose potential problem.\n",
    "sns.lineplot(\n",
    "    x=\"total_timesteps\", y=\"performance\", data=ac_result_lander\n",
    ").set_title(\"Episode Reward in LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "# You should see a pop up window which display the movement of the cart and pole.\n",
    "print(\"Average episode reward for your Actor-critic agent in LunarLander-v2: \",\n",
    "      ac_trainer_lander.evaluate(1, render=True))\n",
    "print(\"One small step for agent, one giant leap for mankind.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4.3: Even harder -- train Pong agents\n",
    "\n",
    "(5 / 100 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "e = gym.make(\"PongDeterministic-v4\"); e.reset()\n",
    "frames = []\n",
    "for _ in range(1000):\n",
    "    frames.append(e.render(\"rgb_array\"))\n",
    "    if e.step(e.action_space.sample())[2]: break\n",
    "e.close();animate(frames);del e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this section, we will train agents to play Pong game in Atari environment. The agent control the bat on the right with up or down action to make it hit the table tennis ball to left side. The agent need to beat a rule-based AI.\n",
    "\n",
    "The observation of the environment `PongDeterministic-v4` is an image of 210 X 160 X 3 pixels. We compress the image and flatten it to be a vector with 6400 elements. To do this, we provide you a wrapper class which wrap the actor-critic trainer to fit this environment. You need to implement the save and restore functions.\n",
    "\n",
    "In the assignment 4, we will trained state of the art Pong agents with various algorithms. Your agents will battle with other AI. Let's get familiar with Pong by finishing this section!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "def process_img(img):\n",
    "    \"\"\"In this function, we process the raw image array into a flat vector.\n",
    "    We assume the img has four dimension, with shape in [Batch size, Width, Height, Channel].\n",
    "    Therefore the raw image is a 4D matrix containing the intensity of colors.\n",
    "    \"\"\"\n",
    "    assert img.shape[-3:] == (210, 160, 3), img.shape\n",
    "    \n",
    "    # Add extra axis if input is a single image and make it looks like a batch.\n",
    "    single_img = False\n",
    "    if img.ndim == 3:\n",
    "        img = img[None, ...]\n",
    "        single_img = True\n",
    "        \n",
    "    # Crop the image\n",
    "    img = img[:, 35:195]\n",
    "    \n",
    "    # Shrink the image size by taking only 1/4 of the pixel\n",
    "    img = img[:, ::2, ::2, 0]\n",
    "    \n",
    "    # Erase background\n",
    "    img[img == 144] = 0\n",
    "    img[img == 109] = 0\n",
    "    \n",
    "    # Paint everything (paddles, ball) to 1\n",
    "    img[img != 0 ] = 1\n",
    "    \n",
    "    # Flatten image to vector\n",
    "    img = img.astype(np.float).reshape(-1, 6400)\n",
    "    \n",
    "    # Reverse single image\n",
    "    if single_img:\n",
    "        img = img[0, ...]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the TODOs and remove `pass`\n",
    "\n",
    "def pong_wrapper(trainer):\n",
    "    assert inspect.isclass(trainer)\n",
    "    \n",
    "    class WrappedTrainer(trainer):\n",
    "        def __init__(self, *a, **k):\n",
    "            super().__init__(*a, **k)\n",
    "            self.obs_dim = 6400\n",
    "            self.build_model()\n",
    "            assert \"Pong\" in self.config[\"env_name\"]\n",
    "            \n",
    "        def compute_action(self, obs):\n",
    "            obs = process_img(obs)\n",
    "            return super().compute_action(obs)\n",
    "            \n",
    "        def process_samples(self, samples):\n",
    "            samples[\"obs\"] = [process_img(traj_obs) for traj_obs in samples[\"obs\"]]\n",
    "            return super().process_samples(samples)\n",
    "        \n",
    "        def train(self):\n",
    "            ret = super().train()\n",
    "            if self.iteration % self.config[\"evaluate_interval\"] == 0:\n",
    "                self.save(\"iter{}\".format(self.iteration))\n",
    "                print(\"Finished {} iterations training. \"\n",
    "                      \"Checkpoint is saved in checkpoints directory.\".format(self.iteration))\n",
    "            return ret\n",
    "        \n",
    "        def save(self, surfix=\"checkpoint\"):\n",
    "            import pickle\n",
    "            import os\n",
    "            os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "            policy_file_name = \"checkpoints/pong-agent-{}-policy.pkl\".format(surfix)\n",
    "            with open(policy_file_name, \"wb\") as f:\n",
    "                # [TODO] Use function of pickle package and torch model to save \n",
    "                # the parameters of self.policy to file \"f\".\n",
    "                # Hint: a function called state_dict of torch model may help.\n",
    "                pass\n",
    "            \n",
    "            baseline_file_name = \"checkpoints/pong-agent-{}-baseline.pkl\".format(surfix)\n",
    "            with open(baseline_file_name, \"wb\") as f:\n",
    "                # [TODO] Use function of pickle package and torch model to save \n",
    "                # the parameters of self.baseline to file \"f\".\n",
    "                pass\n",
    "                    \n",
    "        def restore(self, surfix=\"checkpoint\"):\n",
    "            import pickle\n",
    "            policy_file_name = \"checkpoints/pong-agent-{}-policy.pkl\".format(surfix)\n",
    "            with open(policy_file_name, \"rb\") as f:\n",
    "                # [TODO] Use function of pickle package and torch model to restore \n",
    "                # the parameters of self.policy from file \"f\".\n",
    "                # Hint: a function called load_state_dict of torch model may help.\n",
    "                pass\n",
    "                \n",
    "            baseline_file_name = \"checkpoints/pong-agent-{}-baseline.pkl\".format(surfix)\n",
    "            with open(baseline_file_name, \"rb\") as f:\n",
    "                # [TODO] Use function of pickle package and torch model to restore \n",
    "                # the parameters of self.baseline from file \"f\".\n",
    "                pass\n",
    "        \n",
    "    return WrappedTrainer\n",
    "\n",
    "\n",
    "# Test save and restore\n",
    "pong_wrapper(ActorCriticTrainer)({\"env_name\": \"Pong-v0\"}).save(\"test\")\n",
    "pong_wrapper(ActorCriticTrainer)({\"env_name\": \"Pong-v0\"}).restore(\"test\")\n",
    "import os\n",
    "os.remove(\"checkpoints/pong-agent-test-baseline.pkl\")\n",
    "os.remove(\"checkpoints/pong-agent-test-policy.pkl\")\n",
    "print(\"Test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this cell without modification\n",
    "\n",
    "pong_ac_config = dict(\n",
    "    # environment\n",
    "    env_name=\"PongDeterministic-v4\",\n",
    "\n",
    "    # model\n",
    "    hidden_units=100,\n",
    "\n",
    "    # training inner loop\n",
    "    learning_rate=0.0005,\n",
    "\n",
    "    # training outer loop\n",
    "    max_iteration=200,\n",
    "    max_episode_length=5000,  # In fact the game end at 10000 steps\n",
    "    train_batch_size=5000,\n",
    "\n",
    "    # evaluation\n",
    "    evaluate_interval=10,\n",
    "    evaluate_num_episodes=10\n",
    ")\n",
    "\n",
    "ac_trainer_pong, ac_result_pong = run(pong_wrapper(ActorCriticTrainer), pong_ac_config, 18.0)\n",
    "\n",
    "# Hint: Performance should greater than -20.0 within one hour.\n",
    "# We do not require you to solve the task, but you need to make your agent achieve > -20 reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surfix = \"iter130\"\n",
    "\n",
    "ac_trainer_pong_restored = pong_wrapper(ActorCriticTrainer)(pong_ac_config)\n",
    "ac_trainer_pong_restored.restore(surfix)\n",
    "\n",
    "print(\"Average episode reward for your Actor-critic agent in PongDeterministic-v4: \",\n",
    "      ac_trainer_pong_restored.evaluate(1, render=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "## Conclusion and Discussion\n",
    "\n",
    "In this assignment, we implement several policy gradient algorithms.\n",
    "\n",
    "It's OK to leave the following cells empty. In the next markdown cell, you can write whatever you like. Like the suggestion on the course, the confusing problems in the assignments, and so on.\n",
    "\n",
    "If you want to do more investigation, feel free to open new cells via `Esc + B` after the next cells and write codes in it, so that you can reuse some result in this notebook. Remember to write sufficient comments and documents to let others know what you are doing.\n",
    "\n",
    "Following the submission instruction in the assignment to submit your assignment to our staff. Thank you!\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
